<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Unknown </title></head><body>
<pre><code>library(irrCAC)
</code></pre>
<h1 id="installation">Installation</h1>
<p>devtools::install_github(“kgwet/irrCAC”)</p>
<h1 id="abstract">Abstract</h1>
<p>The <strong>irrCAC</strong> is an R package that provides several functions for
calculating various chance-corrected agreement coefficients. This
package closely follows the general framework of inter-rater reliability
assessment presented by Gwet (2014). A similar package was developed for
STATA users by Klein (2018).</p>
<p>The functions included in this package can handle 3 types of input data:
(1) the contingency table, (2) the distribution of raters by subject and
by category, (3) the raw data, which is essentially a plain dataset
where each row represents a subject and each column, the ratings
associated with one rater. The list of all datasets containined in this
package can be listed as follows:</p>
<pre><code>  data(package="irrCAC")
</code></pre>
<h1 id="computing-agreement-coefficients">Computing Agreement Coefficients</h1>
<h2 id="computing-agreement-coefficients-from-contingency-tables">Computing agreement Coefficients from Contingency tables</h2>
<p><strong>cont3x3abstractors</strong> is one of 2 datasets included in this package and
that contain rating data from 2 raters organized in the form of a
contingency table. The following r script shows how to compute Cohen’s
kappa, Scott’s Pi, Gwet’s AC<sub>1</sub>, Brennan-Prediger,
Krippendorff’s alpha, and the percent agreement coefficients from this
dataset.</p>
<pre><code>  cont3x3abstractors
#&gt;         Ectopic AIU NIU
#&gt; Ectopic      13   0   0
#&gt; AIU           0  20   7
#&gt; NIU           0   4  56
  kappa2.table(cont3x3abstractors)
#&gt;      coeff.name coeff.val   coeff.se     coeff.ci coeff.pval
#&gt; 1 Cohen's Kappa 0.7964094 0.05891072 (0.68,0.913)      0e+00
  scott2.table(cont3x3abstractors)
#&gt;   coeff.name coeff.val   coeff.se      coeff.ci coeff.pval
#&gt; 1 Scott's Pi 0.7962397 0.05905473 (0.679,0.913)      0e+00
  gwet.ac1.table(cont3x3abstractors)
#&gt;   coeff.name coeff.val   coeff.se      coeff.ci coeff.pval
#&gt; 1 Gwet's AC1 0.8493305 0.04321747 (0.764,0.935)      0e+00
  bp2.table(cont3x3abstractors)
#&gt;         coeff.name coeff.val   coeff.se      coeff.ci coeff.pval
#&gt; 1 Brennan-Prediger     0.835 0.04693346 (0.742,0.928)      0e+00
  krippen2.table(cont3x3abstractors)
#&gt;             coeff.name coeff.val   coeff.se     coeff.ci coeff.pval
#&gt; 1 Krippendorff's Alpha 0.7972585 0.05905473 (0.68,0.914)      0e+00
  pa2.table(cont3x3abstractors)
#&gt;          coeff.name coeff.val   coeff.se      coeff.ci coeff.pval
#&gt; 1 Percent Agreement      0.89 0.03128898 (0.828,0.952)      0e+00
</code></pre>
<p>Suppose that you only want to obtain Gwet’s AC<sub>1</sub> coefficient,
but don’t care about the associated precision measures such as the
standard error, confidence intervals or p-values. You can accomplish
this as follows:</p>
<pre><code>  ac1 &lt;- gwet.ac1.table(cont3x3abstractors)$coeff.val
</code></pre>
<p>Then use the variable ac1 to obtain AC<sub>1</sub> = 0.849.</p>
<p>Another contingency table included in this package is named
<strong>cont3x3abstractors</strong>. You may use it to experiment with the r
functions listed above.</p>
<h2 id="computing-agreement-coefficients-from-the-distribution-of-raters-by-subject-category">Computing agreement coefficients from the distribution of raters by subject &amp; category</h2>
<p>Included in this package is a small dataset named <strong>distrib.6raters</strong>,
which contains the distribution of 6 raters by subject and category.
Each row represents a subject (i.e. a psychiatric patient) and the
number of raters (i.e. psychiatrists) who classified it into each
category used in the inter-rater reliability study. Here is the dataset
and how it can be used to compute the various agreement coefficients:</p>
<pre><code>distrib.6raters
#&gt;    Depression Personality.Disorder Schizophrenia Neurosis Other
#&gt; 1           0                    0             0        6     0
#&gt; 2           0                    3             0        0     3
#&gt; 3           0                    1             4        0     1
#&gt; 4           0                    0             0        0     6
#&gt; 5           0                    3             0        3     0
#&gt; 6           2                    0             4        0     0
#&gt; 7           0                    0             4        0     2
#&gt; 8           2                    0             3        1     0
#&gt; 9           2                    0             0        4     0
#&gt; 10          0                    0             0        0     6
#&gt; 11          1                    0             0        5     0
#&gt; 12          1                    1             0        4     0
#&gt; 13          0                    3             3        0     0
#&gt; 14          1                    0             0        5     0
#&gt; 15          0                    2             0        3     1
gwet.ac1.dist(distrib.6raters)
#&gt;   coeff.name     coeff     stderr      conf.int      p.value        pa
#&gt; 1 Gwet's AC1 0.4448007 0.08418757 (0.264,0.625) 0.0001155927 0.5511111
#&gt;          pe
#&gt; 1 0.1914815
fleiss.kappa.dist(distrib.6raters)
#&gt;      coeff.name     coeff     stderr     conf.int      p.value        pa
#&gt; 1 Fleiss' Kappa 0.4139265 0.08119291 (0.24,0.588) 0.0001622724 0.5511111
#&gt;          pe
#&gt; 1 0.2340741
krippen.alpha.dist(distrib.6raters)
#&gt;             coeff.name     coeff     stderr      conf.int      p.value
#&gt; 1 Krippendorff's Alpha 0.4204384 0.08243228 (0.244,0.597) 0.0001615721
#&gt;          pa        pe
#&gt; 1 0.5560988 0.2340741
bp.coeff.dist(distrib.6raters)
#&gt;         coeff.name     coeff     stderr      conf.int   p.value        pa
#&gt; 1 Brennan-Prediger 0.4388889 0.08312142 (0.261,0.617) 0.0001163 0.5511111
#&gt;    pe
#&gt; 1 0.2
</code></pre>
<p>Once again, you can request a single value from these functions. To get
only Krippendorff’s alpha coefficient without it’s precission measures,
you may proceed as follows:</p>
<pre><code>  alpha &lt;- krippen.alpha.dist(distrib.6raters)$coeff
</code></pre>
<p>The newly-created alpha variable gives the coefficient <em>α</em> = 0.4204384.</p>
<p>Two additional datasets that represent ratings in the form of a
distribution of raters by subject and by category, are included in this
package. These datasets are <strong>cac.dist4cat</strong> and <strong>cac.dist4cat</strong>. Note
that these 2 datasets contain more columns than needed to run the 4
functions presented in this section. Therefore, the columns associated
with the response categories must be extracted from the original
datasets before running the functions. For example, computing Gwet’s
AC<sub>1</sub> coefficient using the <strong>cac.dist4cat</strong> dataset should be
done as follows:</p>
<pre><code>  ac1 &lt;- gwet.ac1.dist(cac.dist4cat[,2:4])$coeff
</code></pre>
<p>Note that the input dataset supplied to the function is
<strong>cac.dist4cat[,2:4]</strong>. That is, only columns 2, 3, and 4 are
extracted from the original datset and used as input data. We know from
the value of the newly created variable that AC<sub>1</sub> = 0.3518903.</p>
<h2 id="computing-agreement-coefficients-from-raw-ratings">Computing agreement coefficients from raw ratings</h2>
<p>One example dataset of raw ratings included in this package is
<strong>cac.raw4raters</strong> and looks like this:</p>
<pre><code>  cac.raw4raters
#&gt;    Rater1 Rater2 Rater3 Rater4
#&gt; 1       1      1     NA      1
#&gt; 2       2      2      3      2
#&gt; 3       3      3      3      3
#&gt; 4       3      3      3      3
#&gt; 5       2      2      2      2
#&gt; 6       1      2      3      4
#&gt; 7       4      4      4      4
#&gt; 8       1      1      2      1
#&gt; 9       2      2      2      2
#&gt; 10     NA      5      5      5
#&gt; 11     NA     NA      1      1
#&gt; 12     NA     NA      3     NA
</code></pre>
<p>As you can see, a dataset of raw ratings is merely a listing of ratings
that the raters assigned to the subjects. Each row is associated with a
single subject.Typically, the same subject would be rated by all or some
of the raters. The dataset <strong>cac.raw4raters</strong> contains some missing
ratings represented by the symbol NA, suggesting that some raters did
not rate all subjects. As a matter of fact, in this particular case, no
rater rated all subjects.</p>
<p>Here is you can compute the various agreement coefficients using the raw
ratings:</p>
<pre><code>pa.coeff.raw(cac.raw4raters)
#&gt; $est
#&gt;          coeff.name        pa pe coeff.val coeff.se  conf.int      p.value
#&gt; 1 Percent Agreement 0.8181818  0 0.8181818  0.12561 (0.542,1) 4.345373e-05
#&gt;       w.name
#&gt; 1 unweighted
#&gt; 
#&gt; $weights
#&gt;      [,1] [,2] [,3] [,4] [,5]
#&gt; [1,]    1    0    0    0    0
#&gt; [2,]    0    1    0    0    0
#&gt; [3,]    0    0    1    0    0
#&gt; [4,]    0    0    0    1    0
#&gt; [5,]    0    0    0    0    1
#&gt; 
#&gt; $categories
#&gt; [1] 1 2 3 4 5
gwet.ac1.raw(cac.raw4raters)
#&gt; $est
#&gt;   coeff.name        pa        pe coeff.val coeff.se  conf.int     p.value
#&gt; 1        AC1 0.8181818 0.1903212   0.77544  0.14295 (0.461,1) 0.000208721
#&gt;       w.name
#&gt; 1 unweighted
#&gt; 
#&gt; $weights
#&gt;      [,1] [,2] [,3] [,4] [,5]
#&gt; [1,]    1    0    0    0    0
#&gt; [2,]    0    1    0    0    0
#&gt; [3,]    0    0    1    0    0
#&gt; [4,]    0    0    0    1    0
#&gt; [5,]    0    0    0    0    1
#&gt; 
#&gt; $categories
#&gt; [1] 1 2 3 4 5
fleiss.kappa.raw(cac.raw4raters)
#&gt; $est
#&gt;      coeff.name        pa        pe coeff.val coeff.se  conf.int
#&gt; 1 Fleiss' Kappa 0.8181818 0.2387153   0.76117  0.15302 (0.424,1)
#&gt;       p.value     w.name
#&gt; 1 0.000419173 unweighted
#&gt; 
#&gt; $weights
#&gt;      [,1] [,2] [,3] [,4] [,5]
#&gt; [1,]    1    0    0    0    0
#&gt; [2,]    0    1    0    0    0
#&gt; [3,]    0    0    1    0    0
#&gt; [4,]    0    0    0    1    0
#&gt; [5,]    0    0    0    0    1
#&gt; 
#&gt; $categories
#&gt; [1] 1 2 3 4 5
krippen.alpha.raw(cac.raw4raters)
#&gt; $est
#&gt;             coeff.name    pa   pe coeff.val coeff.se  conf.int
#&gt; 1 Krippendorff's Alpha 0.805 0.24   0.74342  0.14557 (0.419,1)
#&gt;        p.value     w.name
#&gt; 1 0.0004594257 unweighted
#&gt; 
#&gt; $weights
#&gt;      [,1] [,2] [,3] [,4] [,5]
#&gt; [1,]    1    0    0    0    0
#&gt; [2,]    0    1    0    0    0
#&gt; [3,]    0    0    1    0    0
#&gt; [4,]    0    0    0    1    0
#&gt; [5,]    0    0    0    0    1
#&gt; 
#&gt; $categories
#&gt; [1] 1 2 3 4 5
conger.kappa.raw(cac.raw4raters)
#&gt; $est
#&gt;       coeff.name        pa        pe coeff.val coeff.se  conf.int
#&gt; 1 Conger's Kappa 0.8181818 0.2334252   0.76282  0.14917 (0.435,1)
#&gt;        p.value     w.name
#&gt; 1 0.0003367066 unweighted
#&gt; 
#&gt; $weights
#&gt;      [,1] [,2] [,3] [,4] [,5]
#&gt; [1,]    1    0    0    0    0
#&gt; [2,]    0    1    0    0    0
#&gt; [3,]    0    0    1    0    0
#&gt; [4,]    0    0    0    1    0
#&gt; [5,]    0    0    0    0    1
#&gt; 
#&gt; $categories
#&gt; [1] 1 2 3 4 5
bp.coeff.raw(cac.raw4raters)
#&gt; $est
#&gt;         coeff.name        pa  pe coeff.val coeff.se  conf.int      p.value
#&gt; 1 Brennan-Prediger 0.8181818 0.2   0.77273  0.14472 (0.454,1) 0.0002375609
#&gt;       w.name
#&gt; 1 unweighted
#&gt; 
#&gt; $weights
#&gt;      [,1] [,2] [,3] [,4] [,5]
#&gt; [1,]    1    0    0    0    0
#&gt; [2,]    0    1    0    0    0
#&gt; [3,]    0    0    1    0    0
#&gt; [4,]    0    0    0    1    0
#&gt; [5,]    0    0    0    0    1
#&gt; 
#&gt; $categories
#&gt; [1] 1 2 3 4 5
</code></pre>
<p>Most users of this package will only be interessted in the agreement
coefficients and possibly in the related statistics such as the standard
error and p-values. In this case, you should run these functions as
follows (AC<sub>1</sub> is used here as an example. Feel free to
experiment with the other coefficients):</p>
<pre><code>ac1 &lt;- gwet.ac1.raw(cac.raw4raters)$est
ac1
#&gt;   coeff.name        pa        pe coeff.val coeff.se  conf.int     p.value
#&gt; 1        AC1 0.8181818 0.1903212   0.77544  0.14295 (0.461,1) 0.000208721
#&gt;       w.name
#&gt; 1 unweighted
</code></pre>
<p>You can even request only the AC<sub>1</sub> coefficient estimate
0.77544. You will then proceed as follows:</p>
<pre><code>ac1 &lt;- gwet.ac1.raw(cac.raw4raters)$est
ac1$coeff.val
#&gt; [1] 0.77544
</code></pre>
<h1 id="references">References:</h1>
<ol>
<li>Gwet, K.L. (2014) <em>Handbook of Inter-Rater Reliability</em>, 4th
    Edition. Advanced Analytics, LLC.</li>
<li>Klein, D. (2018) “Implementing a general framework for assessing
    interrater agreement in Stata,” , <strong>18</strong>, 871-901.</li>
</ol>
</body></html>